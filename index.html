<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-81852645-3"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-81852645-3');
  </script>

  <title>Manan Tomar</title>

  <meta name="author" content="Manan Tomar">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/png.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Manan Tomar</name>
              </p>
              <p>I am an AI resident at <a href="https://ai.facebook.com/">Facebook AI Research</a>, working in Reinforcement Learning under <a href="http://chercheurs.lille.inria.fr/~ghavamza/my_website/About_Me.html"> Mohammad Ghavamzadeh</a>.
              </p>
              <p>
                Previosly I was at IIT Madras, where I was advised by <a href="https://www.cse.iitm.ac.in/~ravi/"> Prof. Balaraman Ravindran. </a>
              </p>
              <p style="text-align:center">
                <a href="mailto:manan.tomar@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=kamjbL0AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/manan-tomar-615653119/"> LinkedIn </a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Manan_Tomar_cropped.png"><img style="width:100%;max-width:80%" alt="profile photo" src="images/Manan_Tomar_cropped.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                <!-- I'm interested in Reinforcement Learning, Optimization, and Robotic Learning. -->
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle;max-width:40%">
              <!-- <img src="" alt="clean-usnob" width="160" height="100"> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/1910.02919.pdf">
                <papertitle>Multi-step Greedy Policies in Model-Free Deep Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Manan Tomar*</strong>, <a href="">Yonathan Efroni*</a>, <a href="">Mohammad Ghavamzadeh</a>
              <br>
              <em>Under Review at the International Conference on Learning Representations</em>, ICLR 2020
              <p>We explore the benefits of multi-step greedy policies in model-free RL when employed in the framework
								of multi-step Dynamic Programming (DP): multi-step Policy and Value Iteration. These algorithms
								iteratively solve short-horizon decision problems and converge to the optimal solution of the original
								one. By using model-free algorithms as solvers of the short-horizon problems we derive fully model-free
								algorithms which are instances of the multi-step DP framework.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;max-width:40%">
              <img src='images/SROptions_Icon.png' width="160" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1905.05731.pdf">
                <papertitle>Successor Options : An Option Discovery Algorithm for Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Manan Tomar*</strong>,
							<a href="">Rahul Ramesh*</a>,
              <a href="">Balaraman Ravindran</a>,
              <br>
              <em>Accepted at the 28th International Joint Conference on Artificial Intelligence</em>, IJCAI 2019
              <br>
              <p></p>
              <p>We attempt to discover "landmark" sub-goals which are prototypical states of well connected regions. These sub-goals are points from which densely connected set of states are easily accessible.
							</p>
						</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;max-width:40%">
              <img src="images/MaMiC_Icon.png" alt="clean-usnob" width="160" height="100">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/1905.07193.pdf">
                <papertitle>MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning</papertitle>
              </a>
              <br>
              <strong>Manan Tomar</strong>, <a href="">Akhil Sathuluri</a>, <a href="">Balaraman Ravindran</a>
              <br>
              <em>Accepted at the International Conference on Autonomous Agents and Multiagent Systems</em>, AAMAS 2019
              <p>We propose a dual curriculum scheme for solving
							robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro
							curriculum scheme which divides the task into multiple subtasks followed by a
							micro curriculum scheme which enables the agent to learn between such discovered
							subtasks. We show how combining macro and micro curriculum strategies help in
							overcoming major exploratory constraints considered in robot manipulation tasks
							without having to engineer any complex rewards.</p>
            </td>
          </tr>

					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle;max-width:40%">
              <img src="images/MARL_Icon.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1VBZl4GTlL-Hwl2iL8p-EJkE1gv3RVt3R/view?usp=sharing">
                <papertitle>Multi-Agent Reinforcement Learning using Graph Neural Networks</papertitle>
              </a>
              <br>
              <a href="">Rohan Saphal*</a>, <strong>Manan Tomar*</strong>, <a href="">Balaraman Ravindran</a>
              <br>
              <em>Preprint</em>
              <p>We demonstrate that modelling the multi-agent environment in a graph network paradigm can result
								in performance that is comparable or better than baseline deep reinforcement learning
								algorithms. We also show that our framework scales well with increase in the
								number of agents.</p>
            </td>
          </tr>

					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle;max-width:40%">
              <img src="images/SRSurvey_Icon.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1RynGuSgBclTc7c-fJ2COtlHzybVvItWm/view?usp=sharing">
                <papertitle>A Survey on Successor Representation</papertitle>
              </a>
              <br>
              <strong>Manan Tomar</strong>, <a href="">Balaraman Ravindran</a>
              <br>
              <em>Preprint</em>
              <p>This work presents the major ideas in Successor
								Representation and tries to provide a concise conclusion towards future directions.</p>
            </td>
          </tr>

					<tr>
            <td style="padding:20px;width:25%;vertical-align:middle;max-width:40%">
              <img src="images/Quantum_Icon.png" alt="clean-usnob" width="160" height="160">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://drive.google.com/file/d/1ePTa7dO9l3oNAEQAIgWk_QAklN3XMVQ3/view?usp=sharing">
                <papertitle>Quantum Entanglement in Learning Representations</papertitle>
              </a>
              <br>
              <a href="">Ashish Kumar*</a>, <strong>Manan Tomar</strong>, <a href="">Balaraman Ravindran</a>
              <br>
              <em>Preprint</em>
              <p>Often we want to learn the representation of the data such that given an
								observation we can decompose it into independently controllable factors. The field of Quantum mechanics has the
								notion of entanglement which gives entanglement measure as the degree of correlation present in a
								multi particle system. The problem in hand is to use such a measure to show disentanglement
								in semantic space systems.</p>
            </td>
          </tr>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/"> Site Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
