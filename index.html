---
layout: default
title: Manan Tomar, Senior Undergradratuate, IIT Madras
---
<div class="blurb">
	<h1>Hi, welcome to my webpage</h1>
	<p>I am Manan, currently a final year Undergraduate pursuing Engineering Design dual degree at the Indian Institute of Technology, Madras. I am working under
	<a href="https://www.cse.iitm.ac.in/~ravi/index.html">Dr. Balaraman Ravindran</a> primarily in deep reinforcement learning.

You can contact me through mail at manan.tomar@gmail.com</a></p>

	<br>
	<br>
	<h2> Research </h2>
	<br>
	<h4> Successor Options : An Option Discovery Algorithm for Reinforcement Learning <a href="https://drive.google.com/file/d/1XK4LyIi_ES7euSBoDHs235f1XdPD5-to/view?usp=sharing"> [pdf] </a></h4>
	<h5 style="color:#999;"> Manan Tomar*, Rahul Ramesh*, Balaraman Ravindran </h5>
	<h6> Under review at International Conference on Learning Representations (ICLR)-19  </h6>
	<p style="font-size:80%;"> Hierarchical Reinforcement Learning is a popular method to exploit temporal abstractions in order to tackle the curse of dimensionality.
		The options framework is one such hierarchical framework that models the notion of skills or options. However, learning a collection of task-agnostic transferable skills is a challenging task. Option discovery typically entails using heuristics, the majority of which revolve around discovering bottleneck states.
		In this work, we adopt a method complementary to the idea of discovering bottlenecks.
		Instead, we attempt to discover ``landmark" sub-goals which are prototypical states of well connected regions. These sub-goals are points from which densely connected set of states are easily accessible. We propose a new model called Successor options that leverages Successor Representations to achieve the same.
		We also design a novel pseudo-reward for learning the intra-option policies.
		Additionally, we describe an Incremental Successor options model that iteratively builds options and explores in environments where exploration through primitive actions is inadequate to form the Successor Representations. Finally, we demonstrate the efficacy of our approach on a collection of grid worlds and on complex high dimensional environments like Deepmind-Lab. </p>

	<br>

	<h4> MaMiC: Macro and Micro Curriculum for Robotic Reinforcement Learning <a href="https://drive.google.com/file/d/1c4E3lC_ESWjdV5N7xGy3RsyZnFoybXor/view?usp=sharing"> [pdf] </a></h4>
	<h5 style="color:#999;"> Manan Tomar, Akhil Sathuluri, Balaraman Ravindran </h5>
	<h6> Accepted at the thirty-third AAAI Conference (AAAI student abstract version)-19 </h6>
	<p style="font-size:80%;"> Shaping in humans and animals has been shown to be a powerful tool for learning
	complex tasks as compared to learning in a randomized fashion. This makes
	the problem less complex and enables one to solve the easier sub task at hand
	first. Generating a curriculum for such guided learning involves subjecting the
	agent to easier goals first, and then gradually increasing their difficulty. This
	paper takes a similar direction and proposes a dual curriculum scheme for solving
	robotic manipulation tasks with sparse rewards, called MaMiC. It includes a macro
	curriculum scheme which divides the task into multiple subtasks followed by a
	micro curriculum scheme which enables the agent to learn between such discovered
	subtasks. We show how combining macro and micro curriculum strategies help in
	overcoming major exploratory constraints considered in robot manipulation tasks
	without having to engineer any complex rewards. We also illustrate the meaning
	of the individual curricula and how they can be used independently based on the
	task. The performance of such a dual curriculum scheme is analyzed on the Fetch
	environments. </p>

	<br>

	<h4> Multi-Agent Reinforcement Learning using Graph Neural Networks <a href="https://drive.google.com/file/d/1VBZl4GTlL-Hwl2iL8p-EJkE1gv3RVt3R/view?usp=sharing"> [pdf] </a></h4>
	<h5 style="color:#999;"> Manan Tomar*, Rohan Saphal*, Balaraman Ravindran </h5>
	<h6> Pre-print </h6>
	<p style="font-size:80%;"> Multi-agent reinforcement learning (MARL) has seen
		considerable developments over the past few years solving problems across
		a plethora of complex domains. Deep Reinforcement learning algorithms tend to
		perform poorly in environments that require multiple agents to coordinate,
		cooperate and compete with each other. MARL has potential applications across
		a variety of domains such as autonomous driving, robotic control, financial trading,
		etc. Modelling MARL in a structured way can result in significant improvement
		in performance. With the recent developments in Graph Networks, we demonstrate
		that modelling the multi-agent environment in a graph network paradigm can result
		in performance that is comparable or better than baseline deep reinforcement learning
		algorithms. We also show that our framework scales well with increase in the
		number of agents. </p>

		<br>

		<h4> A Survey on Successor Representation <a href="https://drive.google.com/file/d/1RynGuSgBclTc7c-fJ2COtlHzybVvItWm/view?usp=sharing"> [pdf] </a></h4>
		<h5 style="color:#999;"> Manan Tomar, Balaraman Ravindran </h5>
		<h6> Pre-print </h6>
		<p style="font-size:80%;"> Tolman’s experiments on rats trying to find food in a
			maze argue that instead of strengthening and weakening connections for stimuli that
			correspond to positive and negative reinforcement respectively, rats try to build a
			cognitive map of the environment. This cognitive map is able to aid in learning even
			when no food is provided. There have been findings that such a map indeed is built
			in the hippocampus of a rat trying to explore its environment. The Successor Representation
			has been shown to support a similar idea and provide a middle ground between model based
			and model free reinforcement learning. This work presents the major ideas in Successor
			Representation and tries to provide a concise conclusion towards future directions. </p>

		<br>

		<h4> Quantum Entanglement in Learning Representations <a href="https://drive.google.com/file/d/1ePTa7dO9l3oNAEQAIgWk_QAklN3XMVQ3/view?usp=sharing"> [pdf] </a></h4>
		<h5 style="color:#999;"> Ashish Kumar*, Manan Tomar*, Sutanu Chakraborti </h5>
		<h6> Pre-print </h6>
		<p style="font-size:80%;"> Often we want to learn the representation of the data such that given an
			observation we can decompose it into independently controllable factors. In a way, we want a
			disentangled representation of the features which control the observations in the data. In terms
			of image data, we wish to learn the ”objectness” present in the image ie. try to understand the
			generative factors responsible for describing any object. The field of Quantum mechanics has the
			notion of entanglement which gives entanglement measure as the degree of correlation present in a
			multi particle system. The problem in hand is to use such a measure to show disentanglement
			in semantic space systems. </p>

</div><!-- /.blurb -->
